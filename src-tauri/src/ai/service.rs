use super::models::{
    AICompletionRequest, AIRewriteRequest, AIMessage, AIRequest,
    AIGenerateCharacterRequest, AIGenerateCharacterRelationsRequest,
    AIGenerateWorldViewRequest, AIGeneratePlotPointsRequest,
    AIGenerateStoryboardRequest, AIFormatContentRequest,
};
use super::{
    ModelRegistry, PromptManager, BigModelAdapter,
    GeneratorPrompts, FormatOptions,
    GeneratedCharacter, GeneratedCharacterRelation,
    GeneratedWorldView, GeneratedPlotPoint, GeneratedStoryboard,
};
use crate::logger::Logger;
use futures::StreamExt;
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;

pub struct AIService {
    model_registry: ModelRegistry,
    prompt_manager: PromptManager,
    logger: Logger,
}

impl AIService {
    pub fn new() -> Self {
        Self {
            model_registry: ModelRegistry::new(),
            prompt_manager: PromptManager::new(),
            logger: Logger::new().with_feature("ai-service"),
        }
    }

    pub async fn initialize_default_models(&mut self) {
        let default_api_key = std::env::var("BIGMODEL_API_KEY")
            .unwrap_or_else(|_| "45913d02a609452b916a1706b8dc9702".to_string());

        self.logger.info("Initializing default BigModel models");

        let glm4 = Arc::new(BigModelAdapter::new(default_api_key.clone(), "glm-4".to_string()));
        let glm4_plus = Arc::new(BigModelAdapter::new(default_api_key.clone(), "glm-4-plus".to_string()));
        let glm4_air = Arc::new(BigModelAdapter::new(default_api_key.clone(), "glm-4-air".to_string()));
        let glm4_flash = Arc::new(BigModelAdapter::new(default_api_key.clone(), "glm-4-flash".to_string()));
        let glm4_flashx = Arc::new(BigModelAdapter::new(default_api_key.clone(), "glm-4-flashx".to_string()));

        self.model_registry.register_model("glm-4".to_string(), glm4).await;
        self.model_registry.register_model("glm-4-plus".to_string(), glm4_plus).await;
        self.model_registry.register_model("glm-4-air".to_string(), glm4_air).await;
        self.model_registry.register_model("glm-4-flash".to_string(), glm4_flash).await;
        self.model_registry.register_model("glm-4-flashx".to_string(), glm4_flashx).await;

        self.logger.info("Default BigModel models initialized successfully");
    }

    pub fn get_registry(&self) -> &ModelRegistry {
        &self.model_registry
    }

    pub fn get_prompt_manager(&self) -> &PromptManager {
        &self.prompt_manager
    }

    fn clean_json_response(&self, response: &str) -> String {
        let cleaned = response
            .trim()
            .trim_start_matches("```json")
            .trim_start_matches("```")
            .trim_end_matches("```")
            .trim();
        
        cleaned
            .chars()
            .filter(|c| (*c as u32) >= 0x20)
            .collect()
    }

    pub async fn complete(
        &self,
        model_id: &str,
        system_prompt: &str,
        user_content: &str,
    ) -> Result<String, String> {
        let model = self
            .model_registry
            .get_model(model_id)
            .await
            .ok_or_else(|| format!("Model not found: {}", model_id))?;

        let request = AIRequest {
            model: model.get_name(),
            messages: vec![
                AIMessage {
                    role: "system".to_string(),
                    content: system_prompt.to_string(),
                },
                AIMessage {
                    role: "user".to_string(),
                    content: user_content.to_string(),
                },
            ],
            temperature: Some(0.7),
            max_tokens: Some(2000),
            stream: Some(false),
        };

        let response = model.complete(request).await?;
        Ok(response.content)
    }

    pub async fn complete_stream(
        &self,
        model_id: &str,
        system_prompt: &str,
        user_content: &str,
        on_chunk: Box<dyn Fn(String) + Send + Sync>,
    ) -> Result<(), String> {
        let model = self
            .model_registry
            .get_model(model_id)
            .await
            .ok_or_else(|| format!("Model not found: {}", model_id))?;

        let request = AIRequest {
            model: model.get_name(),
            messages: vec![
                AIMessage {
                    role: "system".to_string(),
                    content: system_prompt.to_string(),
                },
                AIMessage {
                    role: "user".to_string(),
                    content: user_content.to_string(),
                },
            ],
            temperature: Some(0.7),
            max_tokens: Some(2000),
            stream: Some(true),
        };

        let mut stream = model.complete_stream(request).await?;

        while let Some(chunk_result) = stream.next().await {
            match chunk_result {
                Ok(chunk) => {
                    if !chunk.content.is_empty() {
                        on_chunk(chunk.content);
                    }
                    if chunk.done {
                        break;
                    }
                }
                Err(e) => {
                    self.logger.error(&format!("Stream error: {}", e));
                    return Err(e);
                }
            }
        }

        Ok(())
    }

    pub async fn continue_novel(
        &self,
        request: AICompletionRequest,
        on_chunk: Option<Box<dyn Fn(String) + Send + Sync>>,
    ) -> Result<String, String> {
        self.logger.info(&format!("Starting novel continuation with model: {}", request.model_id));

        let character_context = request.character_context.clone().unwrap_or_else(|| "æš‚æ— è§’è‰²ä¿¡æ¯".to_string());
        let worldview_context = request.worldview_context.clone().unwrap_or_else(|| "æš‚æ— ä¸–ç•Œè§‚è®¾å®š".to_string());

        let (system_prompt, user_prompt) = self
            .prompt_manager
            .build_prompt(
                "novel-continuation",
                &HashMap::from([
                    ("context".to_string(), request.context),
                    ("instruction".to_string(), request.instruction),
                    ("character_context".to_string(), character_context),
                    ("worldview_context".to_string(), worldview_context),
                ]),
            )
            .await?;

        if let Some(on_chunk) = on_chunk {
            self.complete_stream(&request.model_id, &system_prompt, &user_prompt, on_chunk)
                .await?;
            Ok(String::new())
        } else {
            self.complete(&request.model_id, &system_prompt, &user_prompt)
                .await
        }
    }

    pub async fn rewrite_content(
        &self,
        request: AIRewriteRequest,
    ) -> Result<String, String> {
        self.logger.info(&format!("Starting content rewrite with model: {}", request.model_id));

        let (system_prompt, user_prompt) = self
            .prompt_manager
            .build_prompt(
                "novel-rewrite",
                &HashMap::from([
                    ("content".to_string(), request.content),
                    ("instruction".to_string(), request.instruction),
                ]),
            )
            .await?;

        self.complete(&request.model_id, &system_prompt, &user_prompt)
            .await
    }

    pub async fn generate_dialogue(
        &self,
        model_id: &str,
        character_info: &str,
        scene: &str,
        instruction: &str,
    ) -> Result<String, String> {
        let (system_prompt, user_prompt) = self
            .prompt_manager
            .build_prompt(
                "character-dialogue",
                &HashMap::from([
                    ("character_info".to_string(), character_info.to_string()),
                    ("scene".to_string(), scene.to_string()),
                    ("instruction".to_string(), instruction.to_string()),
                ]),
            )
            .await?;

        self.complete(model_id, &system_prompt, &user_prompt)
            .await
    }

    pub async fn describe_scene(
        &self,
        model_id: &str,
        scene: &str,
        instruction: &str,
    ) -> Result<String, String> {
        let (system_prompt, user_prompt) = self
            .prompt_manager
            .build_prompt(
                "scene-description",
                &HashMap::from([
                    ("scene".to_string(), scene.to_string()),
                    ("instruction".to_string(), instruction.to_string()),
                ]),
            )
            .await?;

        self.complete(model_id, &system_prompt, &user_prompt)
            .await
    }

    pub async fn suggest_plot(
        &self,
        model_id: &str,
        context: &str,
        instruction: &str,
    ) -> Result<String, String> {
        let (system_prompt, user_prompt) = self
            .prompt_manager
            .build_prompt(
                "plot-suggestion",
                &HashMap::from([
                    ("context".to_string(), context.to_string()),
                    ("instruction".to_string(), instruction.to_string()),
                ]),
            )
            .await?;

        self.complete(model_id, &system_prompt, &user_prompt)
            .await
    }

    /// AIç”Ÿæˆè§’è‰²ï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰
    pub async fn generate_character_with_context(
        &self,
        request: AIGenerateCharacterRequest,
        worldviews_context: &str,
        existing_characters_context: &str,
    ) -> Result<GeneratedCharacter, String> {
        self.logger.info(&format!("Starting character generation with context for project: {}", request.project_id));

        let model_id = request.model_id.clone().unwrap_or_else(|| "glm-4-flash".to_string());
        let genre = request.genre.clone().unwrap_or_else(|| "å°è¯´".to_string());

        let system_prompt = r#"ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å°è¯´è§’è‰²è®¾è®¡å¸ˆï¼Œæ“…é•¿åˆ›å»ºç«‹ä½“ã€æœ‰æ·±åº¦çš„è§’è‰²ã€‚

è¯·æ ¹æ®ç”¨æˆ·æä¾›çš„æè¿°å’Œé¡¹ç›®ä¸Šä¸‹æ–‡ï¼Œç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„è§’è‰²è®¾å®šã€‚ä½ éœ€è¦è¿”å›ä¸€ä¸ª JSON æ ¼å¼çš„è§’è‰²æ•°æ®ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š

å¿…å¡«å­—æ®µï¼š
- name: è§’è‰²å§“åï¼ˆå¿…é¡»æœ‰åˆ›æ„ä¸”ç¬¦åˆè®¾å®šï¼‰

å¯é€‰å­—æ®µï¼ˆæ ¹æ®æ•…äº‹éœ€è¦å¡«å†™ï¼‰ï¼š
- role_type: è§’è‰²èº«ä»½ï¼ˆprotagonistä¸»è§’/deuteragonistç¬¬äºŒä¸»è§’/antagoniståæ´¾/supportingé…è§’/minorå°è§’è‰²ï¼‰
- race: ç§æ—ï¼ˆå¦‚äººç±»ã€ç²¾çµã€å…½äººç­‰ï¼Œç¬¦åˆä¸–ç•Œè§‚è®¾å®šï¼‰
- age: å¹´é¾„ï¼ˆæ•´æ•°ï¼‰
- gender: æ€§åˆ«
- birth_date: å‡ºç”Ÿæ—¥æœŸï¼ˆå¦‚"é¾™å†ä¸‰åƒå¹´ä¸‰æœˆåˆä¸‰"è¿™ç§æ•…äº‹å†…çš„æ—¶é—´ï¼‰
- appearance: å¤–è²Œæå†™ï¼ˆ100-200å­—çš„è¯¦ç»†æå†™ï¼‰
- personality: æ€§æ ¼ç‰¹ç‚¹ï¼ˆ100-200å­—ï¼ŒåŒ…å«ä¼˜ç‚¹å’Œç¼ºç‚¹ï¼‰
- background: èƒŒæ™¯æ•…äº‹ï¼ˆ200-300å­—ï¼ŒåŒ…å«æˆé•¿ç»å†å’Œé‡è¦äº‹ä»¶ï¼‰
- mbti: MBTIäººæ ¼ç±»å‹ï¼ˆå¦‚INTJã€ENFPç­‰ï¼Œä»…è¿”å›4ä¸ªå­—æ¯ï¼‰
- enneagram: ä¹å‹äººæ ¼ï¼ˆå¦‚"3å·-æˆå°±å‹"ï¼‰
- bazi: å…«å­—ï¼ˆå¦‚æœæ˜¯ä¸­å¼ç„å¹»/æ­¦ä¾ è®¾å®šï¼‰
- ziwei: ç´«å¾®æ–—æ•°ä¸»è¦æ˜Ÿæ›œé…ç½®ï¼ˆå¦‚æœæ˜¯ä¸­å¼è®¾å®šï¼‰
- skills: æŠ€èƒ½åˆ—è¡¨ï¼ˆç”¨é¡¿å·åˆ†éš”ï¼‰
- status: å½“å‰çŠ¶æ€ï¼ˆå¥åº·ã€æƒ…ç»ªã€ä½ç½®ç­‰ï¼‰
- items: éšèº«é‡è¦ç‰©å“ï¼ˆç”¨é¡¿å·åˆ†éš”ï¼‰

è¯·ç¡®ä¿è§’è‰²å…·æœ‰ï¼š
1. ç‹¬ç‰¹çš„æ€§æ ¼é­…åŠ›å’Œç¼ºç‚¹
2. åˆç†çš„æˆé•¿å¼§çº¿æ½œåŠ›
3. ä¸æ•…äº‹ç±»å‹å’Œä¸–ç•Œè§‚é«˜åº¦å¥‘åˆ
4. ä»¤äººå°è±¡æ·±åˆ»çš„æ ‡å¿—æ€§ç‰¹ç‚¹
5. ä¸å·²æœ‰è§’è‰²å½¢æˆäº’è¡¥æˆ–å†²çªå…³ç³»

åªè¿”å› JSON å¯¹è±¡ï¼Œä¸è¦åŒ…å«markdownä»£ç å—æ ‡è®°æˆ–å…¶ä»–è¯´æ˜æ–‡å­—ã€‚"#;

        let user_prompt = format!(
            r#"è¯·ä¸ºæˆ‘çš„å°è¯´ç”Ÿæˆä¸€ä¸ªè§’è‰²ã€‚

æ•…äº‹ç±»å‹ï¼š{}
è§’è‰²ç±»å‹ï¼š{}
é¢å¤–æè¿°ï¼š{}

=== é¡¹ç›®ä¸Šä¸‹æ–‡ ===

ã€ä¸–ç•Œè§‚è®¾å®šã€‘
{}

ã€å·²æœ‰è§’è‰²ã€‘
{}

è¯·åŸºäºä»¥ä¸Šä¸–ç•Œè§‚å’Œå·²æœ‰è§’è‰²ï¼Œç”Ÿæˆä¸€ä¸ªèƒ½èå…¥è¿™ä¸ªä¸–ç•Œçš„æ–°è§’è‰²ã€‚æ–°è§’è‰²åº”è¯¥ï¼š
1. ç¬¦åˆä¸–ç•Œè§‚è®¾å®šï¼Œç§æ—ã€èƒ½åŠ›ç­‰è¦ä¸ä¸–ç•Œä¸€è‡´
2. ä¸å·²æœ‰è§’è‰²æœ‰æ½œåœ¨çš„äº’åŠ¨å¯èƒ½
3. æœ‰ç‹¬ç‰¹çš„å®šä½ï¼Œä¸ä¸å·²æœ‰è§’è‰²é‡å¤
4. å°½é‡å¡«å†™æ‰€æœ‰å¯å¡«å†™çš„å­—æ®µï¼Œè®©è§’è‰²æ›´åŠ ç«‹ä½“"#,
            genre,
            request.character_type.as_deref().unwrap_or("é…è§’"),
            request.description.as_deref().unwrap_or("æ— ç‰¹æ®Šè¦æ±‚"),
            worldviews_context,
            existing_characters_context
        );

        let response = self.complete(&model_id, system_prompt, &user_prompt).await?;
        
        let cleaned_response = self.clean_json_response(&response);

        let character: GeneratedCharacter = serde_json::from_str(&cleaned_response)
            .map_err(|e| format!("Failed to parse generated character: {}. Response: {}", e, cleaned_response))?;

        self.logger.info(&format!("Character generated successfully: {}", character.name));
        Ok(character)
    }

    /// AIç”Ÿæˆè§’è‰²
    pub async fn generate_character(
        &self,
        request: AIGenerateCharacterRequest,
    ) -> Result<GeneratedCharacter, String> {
        self.logger.info(&format!("Starting character generation for project: {}", request.project_id));

        let model_id = request.model_id.clone().unwrap_or_else(|| "glm-4-flash".to_string());
        let genre = request.genre.clone().unwrap_or_else(|| "å°è¯´".to_string());

        let system_prompt = r#"ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å°è¯´è§’è‰²è®¾è®¡å¸ˆï¼Œæ“…é•¿åˆ›å»ºç«‹ä½“ã€æœ‰æ·±åº¦çš„è§’è‰²ã€‚

è¯·æ ¹æ®ç”¨æˆ·æä¾›çš„æè¿°ï¼Œç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„è§’è‰²è®¾å®šã€‚ä½ éœ€è¦è¿”å›ä¸€ä¸ª JSON æ ¼å¼çš„è§’è‰²æ•°æ®ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š

å¿…å¡«å­—æ®µï¼š
- name: è§’è‰²å§“åï¼ˆå¿…é¡»æœ‰åˆ›æ„ä¸”ç¬¦åˆè®¾å®šï¼‰

å¯é€‰å­—æ®µï¼ˆæ ¹æ®æ•…äº‹éœ€è¦å¡«å†™ï¼‰ï¼š
- role_type: è§’è‰²èº«ä»½ï¼ˆprotagonistä¸»è§’/deuteragonistç¬¬äºŒä¸»è§’/antagoniståæ´¾/supportingé…è§’/minorå°è§’è‰²ï¼‰
- race: ç§æ—ï¼ˆå¦‚äººç±»ã€ç²¾çµã€å…½äººç­‰ï¼‰
- age: å¹´é¾„ï¼ˆæ•´æ•°ï¼‰
- gender: æ€§åˆ«
- birth_date: å‡ºç”Ÿæ—¥æœŸ
- appearance: å¤–è²Œæå†™ï¼ˆ100-200å­—çš„è¯¦ç»†æå†™ï¼‰
- personality: æ€§æ ¼ç‰¹ç‚¹ï¼ˆ100-200å­—ï¼ŒåŒ…å«ä¼˜ç‚¹å’Œç¼ºç‚¹ï¼‰
- background: èƒŒæ™¯æ•…äº‹ï¼ˆ200-300å­—ï¼‰
- mbti: MBTIäººæ ¼ç±»å‹
- enneagram: ä¹å‹äººæ ¼
- skills: æŠ€èƒ½åˆ—è¡¨
- status: å½“å‰çŠ¶æ€
- items: éšèº«ç‰©å“

è¯·ç¡®ä¿è§’è‰²å…·æœ‰ï¼š
1. ç‹¬ç‰¹çš„æ€§æ ¼é­…åŠ›
2. åˆç†çš„æˆé•¿å¼§çº¿æ½œåŠ›
3. ä¸æ•…äº‹ç±»å‹ç›¸ç¬¦çš„ç‰¹å¾
4. ä»¤äººå°è±¡æ·±åˆ»çš„æ ‡å¿—æ€§ç‰¹ç‚¹

åªè¿”å› JSON å¯¹è±¡ï¼Œä¸è¦åŒ…å«markdownä»£ç å—æ ‡è®°æˆ–å…¶ä»–è¯´æ˜æ–‡å­—ã€‚"#;

        let user_prompt = GeneratorPrompts::build_character_prompt(
            &genre,
            request.character_type.as_deref(),
            request.description.as_deref(),
        );

        let response = self.complete(&model_id, system_prompt, &user_prompt).await?;
        
        let cleaned_response = self.clean_json_response(&response);

        let character: GeneratedCharacter = serde_json::from_str(&cleaned_response)
            .map_err(|e| format!("Failed to parse generated character: {}. Response: {}", e, cleaned_response))?;

        self.logger.info(&format!("Character generated successfully: {}", character.name));
        Ok(character)
    }

    /// AIç”Ÿæˆè§’è‰²å…³ç³»
    pub async fn generate_character_relations(
        &self,
        request: AIGenerateCharacterRelationsRequest,
        project_characters: &[crate::models::Character],
        project_context: &str,
    ) -> Result<Vec<GeneratedCharacterRelation>, String> {
        self.logger.info(&format!("Starting character relations generation for project: {}", request.project_id));

        let model_id = request.model_id.clone().unwrap_or_else(|| "glm-4-flash".to_string());

        // æ„å»ºè§’è‰²åˆ—è¡¨å­—ç¬¦ä¸²
        let characters_str = project_characters
            .iter()
            .map(|c| format!("- {} ({}, {}å²): {} - {}", 
                c.name, 
                c.gender.as_deref().unwrap_or("æœªçŸ¥"), 
                c.age.unwrap_or(0),
                c.personality.as_deref().unwrap_or("æ— æ€§æ ¼æè¿°"),
                c.background.as_deref().unwrap_or("æ— èƒŒæ™¯")
            ))
            .collect::<Vec<_>>()
            .join("\n");

        let system_prompt = r#"ä½ æ˜¯ä¸€ä½æ“…é•¿æ„å»ºäººç‰©å…³ç³»çš„å°è¯´ç¼–å‰§ï¼Œèƒ½å¤Ÿè®¾è®¡å‡ºå¤æ‚è€Œåˆç†çš„äººç‰©å…³ç³»ç½‘ç»œã€‚

è¯·æ ¹æ®ç»™å®šçš„è§’è‰²åˆ—è¡¨å’Œæ•…äº‹èƒŒæ™¯ï¼Œç”Ÿæˆè§’è‰²ä¹‹é—´çš„å…³ç³»ç½‘ç»œã€‚è¿”å›ä¸€ä¸ª JSON æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
- from_character_name: è§’è‰²Açš„å§“å
- to_character_name: è§’è‰²Bçš„å§“å
- relation_type: å…³ç³»ç±»å‹ï¼ˆå¦‚ï¼šæœ‹å‹ã€æ•Œäººã€æ‹äººã€å¸ˆå¾’ã€å¯¹æ‰‹ã€äº²äººç­‰ï¼‰
- description: å…³ç³»æè¿°ï¼ˆ50-100å­—ï¼ŒåŒ…å«å…³ç³»èµ·æºå’Œå½“å‰çŠ¶æ€ï¼‰

å…³ç³»ç½‘ç»œè®¾è®¡è¦ç‚¹ï¼š
1. æ¯ä¸ªè§’è‰²åº”è¯¥ä¸å¤šä¸ªå…¶ä»–è§’è‰²æœ‰å…³ç³»ï¼Œå½¢æˆçœŸæ­£çš„ç½‘ç»œ
2. å…³ç³»è¦æœ‰æˆå‰§å¼ åŠ›å’Œå‘å±•ç©ºé—´
3. è¦è€ƒè™‘è§’è‰²æ€§æ ¼çš„å¥‘åˆä¸å†²çª
4. å…³ç³»ç½‘ç»œè¦æœ‰å±‚æ¬¡æ„Ÿå’Œäº¤å‰ç‚¹
5. è¦ä¸ºåç»­æƒ…èŠ‚å‘å±•åŸ‹ä¸‹ä¼ç¬”
6. åŒä¸€ä¸ªè§’è‰²å¯ä»¥æœ‰å¤šç§ä¸åŒç±»å‹çš„å…³ç³»

åªè¿”å› JSON æ•°ç»„ï¼Œä¸è¦åŒ…å«markdownä»£ç å—æ ‡è®°æˆ–å…¶ä»–è¯´æ˜æ–‡å­—ã€‚"#;

        let user_prompt = GeneratorPrompts::build_character_relations_prompt(&characters_str, project_context);

        let response = self.complete(&model_id, system_prompt, &user_prompt).await?;
        
        let cleaned_response = self.clean_json_response(&response);

        let relations: Vec<GeneratedCharacterRelation> = serde_json::from_str(&cleaned_response)
            .map_err(|e| format!("Failed to parse generated relations: {}. Response: {}", e, cleaned_response))?;

        self.logger.info(&format!("Generated {} character relations", relations.len()));
        Ok(relations)
    }

    /// AIç”Ÿæˆä¸–ç•Œè§‚
    pub async fn generate_worldview(
        &self,
        request: AIGenerateWorldViewRequest,
        project_genre: &str,
        existing_worldviews: &[crate::models::WorldView],
    ) -> Result<GeneratedWorldView, String> {
        self.logger.info(&format!("Starting worldview generation for category: {}", request.category));

        let model_id = request.model_id.clone().unwrap_or_else(|| "glm-4-flash".to_string());

        // æ„å»ºå·²æœ‰è®¾å®šå­—ç¬¦ä¸²
        let existing_context = if existing_worldviews.is_empty() {
            "æš‚æ— å·²æœ‰è®¾å®š".to_string()
        } else {
            existing_worldviews
                .iter()
                .map(|w| format!("- [{}] {}: {}", w.category, w.title, w.content.chars().take(100).collect::<String>()))
                .collect::<Vec<_>>()
                .join("\n")
        };

        let system_prompt = r#"ä½ æ˜¯ä¸€ä½ä¸–ç•Œæ„å»ºä¸“å®¶ï¼Œæ“…é•¿åˆ›é€ ç‹¬ç‰¹ã€è‡ªæ´½çš„è™šæ„ä¸–ç•Œã€‚

è¯·æ ¹æ®ç”¨æˆ·æŒ‡å®šçš„ç±»åˆ«ï¼Œç”Ÿæˆä¸–ç•Œè§‚è®¾å®šã€‚è¿”å›ä¸€ä¸ª JSON å¯¹è±¡ï¼ŒåŒ…å«ï¼š
- category: ä¸–ç•Œè§‚ç±»åˆ«ï¼ˆä¸ç”¨æˆ·æŒ‡å®šçš„ç±»åˆ«ä¸€è‡´ï¼‰
- title: è®¾å®šæ ‡é¢˜
- content: è¯¦ç»†å†…å®¹ï¼ˆ300-500å­—ï¼‰
- tags: ç›¸å…³æ ‡ç­¾æ•°ç»„ï¼ˆå¦‚ ["ç„å¹»", "å†å²", "æ˜Ÿè¾°ä¹‹åŠ›"]ï¼‰

ä¸–ç•Œè§‚ç±»åˆ«è¯´æ˜ï¼š
- geography: åœ°ç†ç¯å¢ƒ - åœ°å½¢åœ°è²Œã€æ°”å€™ç‰¹ç‚¹ã€è‡ªç„¶èµ„æº
- history: å†å²èƒŒæ™¯ - é‡è¦äº‹ä»¶ã€æœä»£æ›´è¿­ã€å†å²äººç‰©
- culture: æ–‡åŒ–ä¹ ä¿— - é£ä¿—ä¹ æƒ¯ã€èŠ‚æ—¥åº†å…¸ã€è‰ºæœ¯å½¢å¼
- politics: æ”¿æ²»ä½“åˆ¶ - æƒåŠ›ç»“æ„ã€æ³•å¾‹æ³•è§„ã€æ”¿æ²»æ´¾ç³»
- economy: ç»æµç³»ç»Ÿ - è´§å¸ä½“ç³»ã€è´¸æ˜“å¾€æ¥ã€äº§ä¸šåˆ†å¸ƒ
- religion: å®—æ•™ä¿¡ä»° - ç¥ç¥‡ä½“ç³»ã€ç¥­ç¥€ä»ªå¼ã€ä¿¡ä»°å†²çª
- technology: ç§‘æŠ€æ°´å¹³ - æŠ€æœ¯ç‰¹ç‚¹ã€å‘æ˜åˆ›é€ ã€å‘å±•è¶‹åŠ¿
- magic: é­”æ³•ä½“ç³» - é­”æ³•åŸç†ã€æ–½æ³•æ–¹å¼ã€é™åˆ¶ä»£ä»·
- races: ç§æ—è®¾å®š - ç§æ—ç‰¹ç‚¹ã€ç§æ—å…³ç³»ã€ç§æ—åˆ†å¸ƒ
- organizations: ç»„ç»‡åŠ¿åŠ› - ç»„ç»‡ç›®æ ‡ã€ç»„ç»‡ç»“æ„ã€ç»„ç»‡æ´»åŠ¨

è®¾è®¡è¦ç‚¹ï¼š
1. è¦æœ‰ç‹¬ç‰¹æ€§å’Œè¾¨è¯†åº¦
2. å†…éƒ¨é€»è¾‘è¦è‡ªæ´½
3. è¦ä¸ºæ•…äº‹æä¾›å‘å±•ç©ºé—´
4. è¦æœ‰ç»†èŠ‚æ”¯æ’‘ï¼Œé¿å…ç©ºæ´

åªè¿”å› JSON å¯¹è±¡ï¼Œä¸è¦åŒ…å«markdownä»£ç å—æ ‡è®°æˆ–å…¶ä»–è¯´æ˜æ–‡å­—ã€‚"#;

        let user_prompt = GeneratorPrompts::build_worldview_prompt(
            project_genre,
            &request.category,
            &existing_context,
            request.description.as_deref(),
        );

        let response = self.complete(&model_id, system_prompt, &user_prompt).await?;
        
        let cleaned_response = self.clean_json_response(&response);

        let worldview: GeneratedWorldView = serde_json::from_str(&cleaned_response)
            .map_err(|e| format!("Failed to parse generated worldview: {}. Response: {}", e, cleaned_response))?;

        self.logger.info(&format!("Worldview generated successfully: {}", worldview.title));
        Ok(worldview)
    }

    /// AIç”Ÿæˆä¸–ç•Œè§‚ï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰
    pub async fn generate_worldview_with_context(
        &self,
        request: AIGenerateWorldViewRequest,
        project_genre: &str,
        existing_worldviews: &[crate::models::WorldView],
        characters_context: &str,
        plot_context: &str,
    ) -> Result<GeneratedWorldView, String> {
        self.logger.info(&format!("Starting worldview generation with context for category: {}", request.category));

        let model_id = request.model_id.clone().unwrap_or_else(|| "glm-4-flash".to_string());

        // æ„å»ºå·²æœ‰è®¾å®šå­—ç¬¦ä¸²
        let existing_context = if existing_worldviews.is_empty() {
            "æš‚æ— å·²æœ‰è®¾å®š".to_string()
        } else {
            existing_worldviews
                .iter()
                .map(|w| format!("- [{}] {}: {}", w.category, w.title, w.content.chars().take(100).collect::<String>()))
                .collect::<Vec<_>>()
                .join("\n")
        };

        let system_prompt = r#"ä½ æ˜¯ä¸€ä½ä¸–ç•Œæ„å»ºä¸“å®¶ï¼Œæ“…é•¿åˆ›é€ ç‹¬ç‰¹ã€è‡ªæ´½çš„è™šæ„ä¸–ç•Œã€‚

è¯·æ ¹æ®ç”¨æˆ·æŒ‡å®šçš„ç±»åˆ«å’Œé¡¹ç›®ä¸Šä¸‹æ–‡ï¼Œç”Ÿæˆä¸–ç•Œè§‚è®¾å®šã€‚è¿”å›ä¸€ä¸ª JSON å¯¹è±¡ï¼ŒåŒ…å«ï¼š
- category: ä¸–ç•Œè§‚ç±»åˆ«ï¼ˆä¸ç”¨æˆ·æŒ‡å®šçš„ç±»åˆ«ä¸€è‡´ï¼‰
- title: è®¾å®šæ ‡é¢˜
- content: è¯¦ç»†å†…å®¹ï¼ˆ300-500å­—ï¼‰
- tags: ç›¸å…³æ ‡ç­¾æ•°ç»„ï¼ˆå¦‚ ["ç„å¹»", "å†å²", "æ˜Ÿè¾°ä¹‹åŠ›"]ï¼‰

ä¸–ç•Œè§‚ç±»åˆ«è¯´æ˜ï¼š
- geography: åœ°ç†ç¯å¢ƒ - åœ°å½¢åœ°è²Œã€æ°”å€™ç‰¹ç‚¹ã€è‡ªç„¶èµ„æº
- history: å†å²èƒŒæ™¯ - é‡è¦äº‹ä»¶ã€æœä»£æ›´è¿­ã€å†å²äººç‰©
- culture: æ–‡åŒ–ä¹ ä¿— - é£ä¿—ä¹ æƒ¯ã€èŠ‚æ—¥åº†å…¸ã€è‰ºæœ¯å½¢å¼
- politics: æ”¿æ²»ä½“åˆ¶ - æƒåŠ›ç»“æ„ã€æ³•å¾‹æ³•è§„ã€æ”¿æ²»æ´¾ç³»
- economy: ç»æµç³»ç»Ÿ - è´§å¸ä½“ç³»ã€è´¸æ˜“å¾€æ¥ã€äº§ä¸šåˆ†å¸ƒ
- religion: å®—æ•™ä¿¡ä»° - ç¥ç¥‡ä½“ç³»ã€ç¥­ç¥€ä»ªå¼ã€ä¿¡ä»°å†²çª
- technology: ç§‘æŠ€æ°´å¹³ - æŠ€æœ¯ç‰¹ç‚¹ã€å‘æ˜åˆ›é€ ã€å‘å±•è¶‹åŠ¿
- magic: é­”æ³•ä½“ç³» - é­”æ³•åŸç†ã€æ–½æ³•æ–¹å¼ã€é™åˆ¶ä»£ä»·
- races: ç§æ—è®¾å®š - ç§æ—ç‰¹ç‚¹ã€ç§æ—å…³ç³»ã€ç§æ—åˆ†å¸ƒ
- organizations: ç»„ç»‡åŠ¿åŠ› - ç»„ç»‡ç›®æ ‡ã€ç»„ç»‡ç»“æ„ã€ç»„ç»‡æ´»åŠ¨

è®¾è®¡è¦ç‚¹ï¼š
1. è¦æœ‰ç‹¬ç‰¹æ€§å’Œè¾¨è¯†åº¦
2. å†…éƒ¨é€»è¾‘è¦è‡ªæ´½
3. è¦ä¸ºæ•…äº‹å’Œè§’è‰²æä¾›å‘å±•ç©ºé—´
4. è¦æœ‰ç»†èŠ‚æ”¯æ’‘ï¼Œé¿å…ç©ºæ´
5. è¦ä¸å·²æœ‰è§’è‰²å’Œæƒ…èŠ‚ç›¸å‘¼åº”

åªè¿”å› JSON å¯¹è±¡ï¼Œä¸è¦åŒ…å«markdownä»£ç å—æ ‡è®°æˆ–å…¶ä»–è¯´æ˜æ–‡å­—ã€‚"#;

        let user_prompt = format!(
            r#"è¯·ä¸ºæˆ‘çš„å°è¯´ç”Ÿæˆä¸–ç•Œè§‚è®¾å®šã€‚

æ•…äº‹ç±»å‹ï¼š{}
è®¾å®šç±»åˆ«ï¼š{}
é¢å¤–è¦æ±‚ï¼š{}

=== é¡¹ç›®ä¸Šä¸‹æ–‡ ===

ã€å·²æœ‰ä¸–ç•Œè§‚è®¾å®šã€‘
{}

ã€å·²æœ‰è§’è‰²ã€‘
{}

ã€å·²æœ‰æƒ…èŠ‚ã€‘
{}

è¯·åŸºäºä»¥ä¸Šè§’è‰²å’Œæƒ…èŠ‚ï¼Œç”Ÿæˆèƒ½æ”¯æ’‘æ•…äº‹å‘å±•çš„ä¸–ç•Œè§‚è®¾å®šã€‚è®¾å®šåº”è¯¥ï¼š
1. ä¸ºè§’è‰²æä¾›åˆé€‚çš„æ´»åŠ¨èˆå°
2. ä¸ºæƒ…èŠ‚å‘å±•æä¾›åˆç†çš„èƒŒæ™¯
3. ä¸å·²æœ‰ä¸–ç•Œè§‚è®¾å®šä¿æŒä¸€è‡´
4. å…·æœ‰ç‹¬ç‰¹æ€§å’Œå¸å¼•åŠ›"#,
            project_genre,
            request.category,
            request.description.as_deref().unwrap_or("æ— ç‰¹æ®Šè¦æ±‚"),
            existing_context,
            characters_context,
            plot_context
        );

        let response = self.complete(&model_id, system_prompt, &user_prompt).await?;
        
        let cleaned_response = self.clean_json_response(&response);

        let worldview: GeneratedWorldView = serde_json::from_str(&cleaned_response)
            .map_err(|e| format!("Failed to parse generated worldview: {}. Response: {}", e, cleaned_response))?;

        self.logger.info(&format!("Worldview generated successfully: {}", worldview.title));
        Ok(worldview)
    }

    /// AIç”Ÿæˆæƒ…èŠ‚ç‚¹ï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰
    pub async fn generate_plot_points_with_context(
        &self,
        request: AIGeneratePlotPointsRequest,
        project_info: &str,
        existing_plots: &[crate::models::PlotPoint],
        characters_context: &str,
        worldviews_context: &str,
    ) -> Result<Vec<GeneratedPlotPoint>, String> {
        self.logger.info("Starting plot points generation with context");

        let model_id = request.model_id.clone().unwrap_or_else(|| "glm-4-flash".to_string());

        // æ„å»ºå·²æœ‰æƒ…èŠ‚å­—ç¬¦ä¸²
        let existing_plots_str = if existing_plots.is_empty() {
            "æš‚æ— å·²æœ‰æƒ…èŠ‚".to_string()
        } else {
            existing_plots
                .iter()
                .map(|p| format!("- {}: {}", p.title, p.description.as_deref().unwrap_or("æ— æè¿°")))
                .collect::<Vec<_>>()
                .join("\n")
        };

        let system_prompt = r#"ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å‰§æƒ…è®¾è®¡å¸ˆï¼Œæ“…é•¿è®¾è®¡å¼•äººå…¥èƒœçš„æ•…äº‹æƒ…èŠ‚ã€‚

è¯·æ ¹æ®ç»™å®šçš„æ•…äº‹èƒŒæ™¯ã€è§’è‰²å’Œä¸–ç•Œè§‚ï¼Œç”Ÿæˆæƒ…èŠ‚ç‚¹ã€‚è¿”å›ä¸€ä¸ª JSON æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
- title: æƒ…èŠ‚ç‚¹æ ‡é¢˜ï¼ˆç®€çŸ­æœ‰åŠ›ï¼‰
- description: æƒ…èŠ‚æè¿°ï¼ˆ100-200å­—ï¼Œè¦å…·ä½“æ¶‰åŠè§’è‰²ï¼‰
- note: åˆ›ä½œæç¤ºï¼ˆå¯é€‰ï¼Œ50å­—å†…çš„æ³¨æ„äº‹é¡¹ï¼‰
- emotional_tone: æƒ…æ„ŸåŸºè°ƒï¼ˆå¦‚ï¼šç´§å¼ ã€æ¸©é¦¨ã€æ‚²ä¼¤ã€æ¬¢å¿«ç­‰ï¼‰

æƒ…èŠ‚è®¾è®¡è¦ç‚¹ï¼š
1. è¦æœ‰æ˜ç¡®çš„å› æœå…³ç³»
2. è¦æ¨åŠ¨è§’è‰²æˆé•¿å’Œå…³ç³»å˜åŒ–
3. è¦æœ‰æ„å¤–æ€§å’Œåˆç†æ€§
4. è¦ä¸ºåç»­å‘å±•åŸ‹ä¸‹ä¼ç¬”
5. è¦æœ‰æƒ…æ„Ÿå…±é¸£ç‚¹
6. è¦å……åˆ†åˆ©ç”¨ä¸–ç•Œè§‚è®¾å®š

åªè¿”å› JSON æ•°ç»„ï¼Œä¸è¦åŒ…å«markdownä»£ç å—æ ‡è®°æˆ–å…¶ä»–è¯´æ˜æ–‡å­—ã€‚"#;

        let context = request.context.as_deref().unwrap_or(project_info);
        let user_prompt = format!(
            r#"è¯·ä¸ºæˆ‘çš„å°è¯´ç”Ÿæˆæƒ…èŠ‚ç‚¹ã€‚

é¡¹ç›®ä¿¡æ¯ï¼š{}

ã€å·²æœ‰æƒ…èŠ‚ã€‘
{}

ã€è§’è‰²ä¿¡æ¯ã€‘
{}

ã€ä¸–ç•Œè§‚è®¾å®šã€‘
{}

ã€å‘å±•æ–¹å‘ã€‘
{}

è¯·åŸºäºä»¥ä¸Šè§’è‰²å’Œä¸–ç•Œè§‚ï¼Œç”Ÿæˆèƒ½ä¸è§’è‰²äº§ç”Ÿäº’åŠ¨ã€ç¬¦åˆä¸–ç•Œè§‚çš„æƒ…èŠ‚ã€‚æƒ…èŠ‚åº”è¯¥ï¼š
1. è®©è§’è‰²åœ¨æ•…äº‹ä¸­å‘æŒ¥é‡è¦ä½œç”¨
2. ç¬¦åˆä¸–ç•Œè§‚è®¾å®š
3. æ¨åŠ¨è§’è‰²å…³ç³»å‘å±•
4. ä¸å·²æœ‰æƒ…èŠ‚å½¢æˆè¿è´¯çš„æ•…äº‹çº¿"#,
            context,
            existing_plots_str,
            characters_context,
            worldviews_context,
            request.direction.as_deref().unwrap_or("è‡ªç„¶å‘å±•ï¼Œæ³¨é‡æƒ…æ„Ÿæ·±åº¦")
        );

        let response = self.complete(&model_id, system_prompt, &user_prompt).await?;
        
        let cleaned_response = self.clean_json_response(&response);

        let plot_points: Vec<GeneratedPlotPoint> = serde_json::from_str(&cleaned_response)
            .map_err(|e| format!("Failed to parse generated plot points: {}. Response: {}", e, cleaned_response))?;

        self.logger.info(&format!("Generated {} plot points", plot_points.len()));
        Ok(plot_points)
    }

    /// AIç”Ÿæˆæƒ…èŠ‚ç‚¹
    pub async fn generate_plot_points(
        &self,
        request: AIGeneratePlotPointsRequest,
        project_info: &str,
        existing_plots: &[crate::models::PlotPoint],
    ) -> Result<Vec<GeneratedPlotPoint>, String> {
        self.logger.info(&format!("Starting plot points generation for project"));

        let model_id = request.model_id.clone().unwrap_or_else(|| "glm-4-flash".to_string());

        // æ„å»ºå·²æœ‰æƒ…èŠ‚å­—ç¬¦ä¸²
        let existing_plots_str = if existing_plots.is_empty() {
            "æš‚æ— å·²æœ‰æƒ…èŠ‚".to_string()
        } else {
            existing_plots
                .iter()
                .map(|p| format!("- {}: {}", p.title, p.description.as_deref().unwrap_or("æ— æè¿°")))
                .collect::<Vec<_>>()
                .join("\n")
        };

        let system_prompt = r#"ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å‰§æƒ…è®¾è®¡å¸ˆï¼Œæ“…é•¿è®¾è®¡å¼•äººå…¥èƒœçš„æ•…äº‹æƒ…èŠ‚ã€‚

è¯·æ ¹æ®ç»™å®šçš„æ•…äº‹èƒŒæ™¯å’Œå‘å±•æ–¹å‘ï¼Œç”Ÿæˆæƒ…èŠ‚ç‚¹ã€‚è¿”å›ä¸€ä¸ª JSON æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
- title: æƒ…èŠ‚ç‚¹æ ‡é¢˜ï¼ˆç®€çŸ­æœ‰åŠ›ï¼‰
- description: æƒ…èŠ‚æè¿°ï¼ˆ100-200å­—ï¼‰
- note: åˆ›ä½œæç¤ºï¼ˆå¯é€‰ï¼Œ50å­—å†…çš„æ³¨æ„äº‹é¡¹ï¼‰
- emotional_tone: æƒ…æ„ŸåŸºè°ƒï¼ˆå¦‚ï¼šç´§å¼ ã€æ¸©é¦¨ã€æ‚²ä¼¤ã€æ¬¢å¿«ç­‰ï¼‰

æƒ…èŠ‚è®¾è®¡è¦ç‚¹ï¼š
1. è¦æœ‰æ˜ç¡®çš„å› æœå…³ç³»
2. è¦æ¨åŠ¨è§’è‰²æˆé•¿
3. è¦æœ‰æ„å¤–æ€§å’Œåˆç†æ€§
4. è¦ä¸ºåç»­å‘å±•åŸ‹ä¸‹ä¼ç¬”
5. è¦æœ‰æƒ…æ„Ÿå…±é¸£ç‚¹

åªè¿”å› JSON æ•°ç»„ï¼Œä¸è¦åŒ…å«markdownä»£ç å—æ ‡è®°æˆ–å…¶ä»–è¯´æ˜æ–‡å­—ã€‚"#;

        let context = request.context.as_deref().unwrap_or(project_info);
        let user_prompt = GeneratorPrompts::build_plot_points_prompt(
            context,
            &existing_plots_str,
            request.direction.as_deref(),
        );

        let response = self.complete(&model_id, system_prompt, &user_prompt).await?;
        
        let cleaned_response = self.clean_json_response(&response);

        let plot_points: Vec<GeneratedPlotPoint> = serde_json::from_str(&cleaned_response)
            .map_err(|e| format!("Failed to parse generated plot points: {}. Response: {}", e, cleaned_response))?;

        self.logger.info(&format!("Generated {} plot points", plot_points.len()));
        Ok(plot_points)
    }

    /// AIç”Ÿæˆåˆ†é•œæç¤ºè¯
    pub async fn generate_storyboard(
        &self,
        request: AIGenerateStoryboardRequest,
        content: &str,
    ) -> Result<Vec<GeneratedStoryboard>, String> {
        self.logger.info("Starting storyboard generation");

        let model_id = request.model_id.clone().unwrap_or_else(|| "glm-4-flash".to_string());

        let system_prompt = r#"ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å½±è§†åˆ†é•œå¸ˆå’ŒAIç»˜ç”»æç¤ºè¯ä¸“å®¶ï¼Œèƒ½å¤Ÿå°†æ–‡å­—åœºæ™¯è½¬åŒ–ä¸ºç²¾ç¡®çš„å›¾åƒç”Ÿæˆæç¤ºè¯ã€‚

è¯·æ ¹æ®ç»™å®šçš„åœºæ™¯æè¿°ï¼Œç”Ÿæˆåˆ†é•œæç¤ºè¯ã€‚è¿”å›ä¸€ä¸ª JSON æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
- shot_number: é•œå¤´ç¼–å·ï¼ˆæ•´æ•°ï¼Œä»1å¼€å§‹ï¼‰
- shot_type: é•œå¤´ç±»å‹ï¼ˆç‰¹å†™ã€ä¸­æ™¯ã€è¿œæ™¯ã€ä¿¯è§†ã€ä»°è§†ç­‰ï¼‰
- duration: å»ºè®®æ—¶é•¿ï¼ˆç§’ï¼Œæ•´æ•°ï¼‰
- scene_description: åœºæ™¯æè¿°ï¼ˆä¸­æ–‡ï¼Œ50-100å­—ï¼‰
- camera_movement: é•œå¤´è¿åŠ¨ï¼ˆæ¨ã€æ‹‰ã€æ‘‡ã€ç§»ã€è·Ÿç­‰ï¼‰
- visual_prompt: AIç»˜ç”»æç¤ºè¯ï¼ˆè‹±æ–‡ï¼Œç”¨äºMidjourney/Stable Diffusionï¼ŒåŒ…å«ä¸»ä½“ã€ç¯å¢ƒã€å…‰çº¿ã€é£æ ¼ç­‰ï¼‰
- negative_prompt: è´Ÿé¢æç¤ºè¯ï¼ˆå¯é€‰ï¼Œé¿å…ç”Ÿæˆçš„å†…å®¹ï¼‰
- style_notes: é£æ ¼å¤‡æ³¨ï¼ˆè‰²è°ƒã€å…‰å½±ç­‰ï¼‰

åˆ†é•œè®¾è®¡è¦ç‚¹ï¼š
1. é•œå¤´è¦æœ‰å˜åŒ–å’ŒèŠ‚å¥æ„Ÿ
2. è¦çªå‡ºé‡ç‚¹å’Œæƒ…æ„Ÿ
3. è¦è€ƒè™‘ç”»é¢æ„å›¾å’Œè§†è§‰å†²å‡»
4. AIæç¤ºè¯è¦å…·ä½“ã€å¯æ‰§è¡Œ

åªè¿”å› JSON æ•°ç»„ï¼Œä¸è¦åŒ…å«markdownä»£ç å—æ ‡è®°æˆ–å…¶ä»–è¯´æ˜æ–‡å­—ã€‚"#;

        let user_prompt = GeneratorPrompts::build_storyboard_prompt(
            content,
            request.style_preference.as_deref(),
        );

        let response = self.complete(&model_id, system_prompt, &user_prompt).await?;
        
        let cleaned_response = self.clean_json_response(&response);

        let storyboard: Vec<GeneratedStoryboard> = serde_json::from_str(&cleaned_response)
            .map_err(|e| format!("Failed to parse generated storyboard: {}. Response: {}", e, cleaned_response))?;

        self.logger.info(&format!("Generated {} storyboard shots", storyboard.len()));
        Ok(storyboard)
    }

    /// AIä¸€é”®æ’ç‰ˆ
    pub async fn format_content(
        &self,
        request: AIFormatContentRequest,
    ) -> Result<String, String> {
        self.logger.info("Starting content formatting");

        let model_id = request.model_id.clone().unwrap_or_else(|| "glm-4-flash".to_string());

        let options = FormatOptions {
            paragraph_style: request.paragraph_style.unwrap_or_else(|| "ç©ºè¡Œåˆ†éš”".to_string()),
            dialogue_style: request.dialogue_style.unwrap_or_else(|| "ä¸­æ–‡å¼•å·".to_string()),
            scene_separator: request.scene_separator.unwrap_or_else(|| "***".to_string()),
            special_requirements: request.special_requirements.unwrap_or_else(|| "æ— ".to_string()),
        };

        let system_prompt = r#"ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–‡å­—æ’ç‰ˆç¼–è¾‘ï¼Œæ“…é•¿ä¼˜åŒ–å°è¯´æ–‡æœ¬çš„æ ¼å¼å’Œå¯è¯»æ€§ã€‚

è¯·æ ¹æ®ç”¨æˆ·çš„è¦æ±‚å¯¹æ–‡æœ¬è¿›è¡Œæ’ç‰ˆå¤„ç†ã€‚ä½ éœ€è¦ï¼š
1. ä¿®æ­£æ®µè½æ ¼å¼
2. ä¼˜åŒ–å¯¹è¯æ’ç‰ˆ
3. è°ƒæ•´æ ‡ç‚¹ç¬¦å·
4. å¤„ç†åœºæ™¯è½¬æ¢
5. ç»Ÿä¸€æ ¼å¼é£æ ¼

æ’ç‰ˆè§„åˆ™ï¼š
- æ®µè½ä¹‹é—´ç©ºä¸€è¡Œ
- å¯¹è¯ä½¿ç”¨æ­£ç¡®çš„å¼•å·æ ¼å¼
- åœºæ™¯è½¬æ¢ä½¿ç”¨åˆ†éš”ç¬¦
- å¿ƒç†æ´»åŠ¨ç”¨æ–œä½“æˆ–ç‰¹å®šç¬¦å·æ ‡æ³¨
- åŠ¨ä½œæå†™ç‹¬ç«‹æˆæ®µ

åªè¿”å›æ’ç‰ˆåçš„çº¯æ–‡æœ¬å†…å®¹ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šè¯´æ˜ã€å¼•å·åŒ…è£¹æˆ–markdownä»£ç å—æ ‡è®°ã€‚"#;

        let user_prompt = GeneratorPrompts::build_format_prompt(&request.content, &options);

        let response = self.complete(&model_id, system_prompt, &user_prompt).await?;
        
        // æ¸…ç†å“åº”ï¼Œç§»é™¤å¯èƒ½çš„å¼•å·åŒ…è£¹
        let cleaned_response = response
            .trim()
            .trim_start_matches("```")
            .trim_end_matches("```")
            .trim()
            .trim_matches('"');

        self.logger.info("Content formatted successfully");
        Ok(cleaned_response.to_string())
    }

    /// ç”Ÿæˆç»­å†™é€‰é¡¹
    pub async fn generate_writing_choices(
        &self,
        request: crate::models::GenerateWritingChoicesRequest,
        characters: &[crate::models::Character],
        worldviews: &[crate::models::WorldView],
        plot_points: &[crate::models::PlotPoint],
    ) -> Result<crate::models::WritingSuggestion, String> {
        self.logger.info(&format!("Generating writing choices for chapter: {}", request.chapter_id));

        let model_id = request.model_id.clone().unwrap_or_else(|| "glm-4-flash".to_string());

        // æ„å»ºè§’è‰²ä¸Šä¸‹æ–‡
        let characters_context = characters
            .iter()
            .map(|c| format!("- {} ({}, {}å²): {}", 
                c.name, 
                c.gender.as_deref().unwrap_or("æœªçŸ¥"), 
                c.age.unwrap_or(0),
                c.personality.as_deref().unwrap_or("æ— æè¿°")))
            .collect::<Vec<_>>()
            .join("\n");

        // æ„å»ºä¸–ç•Œè§‚ä¸Šä¸‹æ–‡
        let worldview_context = worldviews
            .iter()
            .take(5)
            .map(|w| format!("- [{}] {}", w.category, w.title))
            .collect::<Vec<_>>()
            .join("\n");

        // æ„å»ºæƒ…èŠ‚ç‚¹ä¸Šä¸‹æ–‡
        let plot_context = plot_points
            .iter()
            .take(5)
            .map(|p| format!("- {}", p.title))
            .collect::<Vec<_>>()
            .join("\n");

        // è·å–å½“å‰å†…å®¹çš„æœ€åéƒ¨åˆ†ä½œä¸ºä¸Šä¸‹æ–‡
        let content_preview = if request.current_content.len() > 500 {
            &request.current_content[request.current_content.len() - 500..]
        } else {
            &request.current_content
        };

        let system_prompt = r#"ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å°è¯´åˆ›ä½œé¡¾é—®ï¼Œæ“…é•¿åˆ†æå‰§æƒ…èµ°å‘å¹¶æä¾›å¤šç§ç»­å†™æ–¹å‘ã€‚

è¯·æ ¹æ®å½“å‰çš„å†™ä½œå†…å®¹ï¼Œè¿”å›ä¸€ä¸ª JSON å¯¹è±¡ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- choices: ä¸€ä¸ªæ•°ç»„ï¼ŒåŒ…å«3-5ä¸ªä¸åŒçš„ç»­å†™æ–¹å‘é€‰é¡¹ï¼Œæ¯ä¸ªé€‰é¡¹åŒ…å«ï¼š
  - id: å”¯ä¸€æ ‡è¯†ï¼ˆå¦‚ "choice_1"ï¼‰
  - direction: æ–¹å‘ç±»å‹ï¼ˆå¦‚ï¼šå†²çªå‡çº§ã€æƒ…æ„Ÿæ·±åŒ–ã€å‰§æƒ…åè½¬ã€å¹³ç¨³è¿‡æ¸¡ã€ç´§å¼ æ‚¬ç–‘ã€å¥‡é‡æœºç¼˜ç­‰ï¼‰
  - direction_icon: æ–¹å‘å›¾æ ‡ï¼ˆå¦‚ï¼šğŸ”¥ã€ğŸ’”ã€ğŸ­ã€ğŸŒŠã€âš¡ã€âœ¨ç­‰emojiï¼‰
  - preview: 100-150å­—çš„ç»­å†™é¢„è§ˆ
  - hint: è¿™ä¸ªé€‰æ‹©å¯èƒ½å¸¦æ¥çš„å½±å“æç¤ºï¼ˆ50å­—ä»¥å†…ï¼‰
  - characters: å°†æ¶‰åŠçš„è§’è‰²åå­—æ•°ç»„
  - emotional_tone: æƒ…æ„ŸåŸºè°ƒï¼ˆå¦‚ï¼šç´§å¼ ã€æ¸©é¦¨ã€æ‚²ä¼¤ã€æ¬¢å¿«ç­‰ï¼‰

- detected_characters: å½“å‰å†…å®¹ä¸­å‡ºç°çš„è§’è‰²åå­—æ•°ç»„
- new_characters: å½“å‰å†…å®¹ä¸­å‡ºç°ä½†ä¸åœ¨å·²æœ‰è§’è‰²åˆ—è¡¨ä¸­çš„åå­—
- consistency_warnings: ä¸€è‡´æ€§è­¦å‘Šæ•°ç»„ï¼Œæ¯ä¸ªåŒ…å«ï¼š
  - warning_type: è­¦å‘Šç±»å‹ï¼ˆå¦‚ï¼šcharacter_personalityã€character_relationã€world_settingç­‰ï¼‰
  - character_name: ç›¸å…³è§’è‰²åï¼ˆå¦‚é€‚ç”¨ï¼‰
  - expected: è®¾å®šä¸­çš„æè¿°
  - actual: å½“å‰å†…å®¹ä¸­çš„æè¿°
  - severity: ä¸¥é‡ç¨‹åº¦ï¼ˆlowã€mediumã€highï¼‰
- new_settings: æ£€æµ‹åˆ°çš„æ–°è®¾å®š/åè¯

ç¡®ä¿æ¯ä¸ªé€‰é¡¹éƒ½æœ‰æ˜æ˜¾çš„å·®å¼‚ï¼Œç»™ä½œè€…æä¾›çœŸæ­£çš„é€‰æ‹©ç©ºé—´ã€‚åªè¿”å› JSON å¯¹è±¡ï¼Œä¸è¦åŒ…å«markdownä»£ç å—æ ‡è®°ã€‚"#;

        let user_prompt = format!(
            r#"è¯·ä¸ºæˆ‘çš„å°è¯´ç”Ÿæˆç»­å†™é€‰é¡¹ã€‚

ã€å·²æœ‰è§’è‰²ã€‘
{}

ã€ä¸–ç•Œè§‚è®¾å®šã€‘
{}

ã€å‰§æƒ…è§„åˆ’ã€‘
{}

ã€å½“å‰å†…å®¹ï¼ˆæœ«å°¾éƒ¨åˆ†ï¼‰ã€‘
{}

è¯·åˆ†æå½“å‰å†…å®¹ï¼Œæ£€æµ‹è§’è‰²ä¸€è‡´æ€§ï¼Œå¹¶æä¾›å¤šä¸ªä¸åŒæ–¹å‘çš„ç»­å†™é€‰é¡¹ã€‚"#,
            characters_context,
            worldview_context,
            plot_context,
            content_preview
        );

        let response = self.complete(&model_id, system_prompt, &user_prompt).await?;
        
        let cleaned_response = self.clean_json_response(&response);

        let suggestion: crate::models::WritingSuggestion = serde_json::from_str(&cleaned_response)
            .map_err(|e| format!("Failed to parse writing suggestion: {}. Response: {}", e, cleaned_response))?;

        self.logger.info(&format!("Generated {} writing choices", suggestion.choices.len()));
        Ok(suggestion)
    }

    /// éªŒè¯å†™ä½œå†…å®¹çš„ä¸€è‡´æ€§
    pub async fn validate_writing(
        &self,
        request: crate::models::ValidateWritingRequest,
        characters: &[crate::models::Character],
        worldviews: &[crate::models::WorldView],
        relations: &[crate::models::CharacterRelation],
    ) -> Result<crate::models::ValidationResult, String> {
        self.logger.info("Validating writing content");

        let model_id = "glm-4-flash".to_string();

        // æ„å»ºè§’è‰²ä¿¡æ¯
        let characters_info = characters
            .iter()
            .map(|c| {
                let relations_str = relations
                    .iter()
                    .filter(|r| r.from_character_id == c.id || r.to_character_id == c.id)
                    .map(|r| {
                        let other_name = if r.from_character_id == c.id {
                            characters.iter().find(|ch| ch.id == r.to_character_id).map(|ch| ch.name.as_str()).unwrap_or("?")
                        } else {
                            characters.iter().find(|ch| ch.id == r.from_character_id).map(|ch| ch.name.as_str()).unwrap_or("?")
                        };
                        format!("{}ï¼ˆ{}ï¼‰", other_name, r.relation_type)
                    })
                    .collect::<Vec<_>>()
                    .join("ã€");

                format!("- {} ({}, {}å²) | æ€§æ ¼: {} | å…³ç³»: {}", 
                    c.name, 
                    c.gender.as_deref().unwrap_or("æœªçŸ¥"), 
                    c.age.unwrap_or(0),
                    c.personality.as_deref().unwrap_or("æ— "),
                    if relations_str.is_empty() { "æ— " } else { &relations_str }
                )
            })
            .collect::<Vec<_>>()
            .join("\n");

        // æ„å»ºä¸–ç•Œè§‚å…³é”®è¯
        let settings_keywords = worldviews
            .iter()
            .flat_map(|w| {
                let mut keywords = vec![w.title.clone()];
                keywords.extend(w.tags.clone().unwrap_or_default().split(',').map(|s| s.trim().to_string()));
                keywords
            })
            .collect::<Vec<_>>()
            .join("ã€");

        // è·å–å†…å®¹çš„æœ€å1000å­—ç¬¦è¿›è¡Œåˆ†æ
        let content_to_check = if request.content.len() > 1000 {
            &request.content[request.content.len() - 1000..]
        } else {
            &request.content
        };

        let system_prompt = r#"ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å°è¯´ç¼–è¾‘ï¼Œæ“…é•¿æ£€æŸ¥æ–‡æœ¬çš„ä¸€è‡´æ€§å’Œè®¾å®šå†²çªã€‚

è¯·åˆ†æç»™å®šçš„æ–‡æœ¬ï¼Œè¿”å›ä¸€ä¸ª JSON å¯¹è±¡ï¼ŒåŒ…å«ï¼š
- detected_characters: æ£€æµ‹åˆ°çš„è§’è‰²æ•°ç»„ï¼Œæ¯ä¸ªåŒ…å«ï¼š
  - name: è§’è‰²å
  - character_id: å¦‚æœåŒ¹é…å·²æœ‰è§’è‰²ï¼Œå¡«å…¥IDï¼Œå¦åˆ™null
  - is_new: æ˜¯å¦æ˜¯æ–°è§’è‰²
  - actions: è§’è‰²åœ¨æ–‡æœ¬ä¸­çš„è¡Œä¸ºæè¿°ï¼ˆç®€è¦ï¼‰
- new_characters: æœªåœ¨å·²æœ‰è§’è‰²åˆ—è¡¨ä¸­çš„è§’è‰²åæ•°ç»„
- consistency_warnings: ä¸€è‡´æ€§é—®é¢˜æ•°ç»„ï¼Œæ¯ä¸ªåŒ…å«ï¼š
  - warning_type: é—®é¢˜ç±»å‹
  - character_name: ç›¸å…³è§’è‰²
  - expected: è®¾å®šæƒ…å†µ
  - actual: æ–‡æœ¬ä¸­çš„æƒ…å†µ
  - severity: ä¸¥é‡ç¨‹åº¦ï¼ˆlow/medium/highï¼‰
- detected_settings: æ–‡æœ¬ä¸­æ¶‰åŠçš„ä¸–ç•Œè§‚è®¾å®š
- new_settings: ä¸åœ¨å·²æœ‰è®¾å®šä¸­çš„æ–°åè¯/è®¾å®š

åªè¿”å› JSON å¯¹è±¡ï¼Œä¸è¦åŒ…å«markdownä»£ç å—æ ‡è®°ã€‚"#;

        let user_prompt = format!(
            r#"è¯·æ£€æŸ¥ä»¥ä¸‹å°è¯´ç‰‡æ®µçš„ä¸€è‡´æ€§ã€‚

ã€å·²æœ‰è§’è‰²åŠè®¾å®šã€‘
{}

ã€ä¸–ç•Œè§‚å…³é”®è¯ã€‘
{}

ã€å¾…æ£€æŸ¥çš„æ–‡æœ¬ã€‘
{}

è¯·æ£€æµ‹è§’è‰²å‡ºåœºã€æ€§æ ¼ä¸€è‡´æ€§ã€å…³ç³»è¡¨ç°ï¼Œä»¥åŠä¸–ç•Œè§‚è®¾å®šçš„ä½¿ç”¨æƒ…å†µã€‚"#,
            characters_info,
            settings_keywords,
            content_to_check
        );

        let response = self.complete(&model_id, system_prompt, &user_prompt).await?;
        
        let cleaned_response = self.clean_json_response(&response);

        let result: crate::models::ValidationResult = serde_json::from_str(&cleaned_response)
            .map_err(|e| format!("Failed to parse validation result: {}. Response: {}", e, cleaned_response))?;

        self.logger.info(&format!("Validation complete: {} characters detected, {} warnings", 
            result.detected_characters.len(), result.consistency_warnings.len()));
        Ok(result)
    }
}

impl Default for AIService {
    fn default() -> Self {
        Self::new()
    }
}

pub type AIServiceArc = Arc<RwLock<AIService>>;

pub fn create_ai_service() -> AIServiceArc {
    Arc::new(RwLock::new(AIService::new()))
}
